## 보 팅 : 서로다른 모델을 사용하여 같은 데이터셋을 학습하여 투표를 통해 최종 예측을 하는 것.

분류(classification)에서 앙상블할 때는 두가지 유형으로 나뉜다.

하드보팅 : 다수결로 결정

소프트보팅 : 각 레이블 확률을 평균내어 결정 - 더 합리적

## 배 깅 : 같은 모델을 사용 다른 데이터셋을 학습, 분류에서는 투표 regression에서는 평균으로 최종예측

boostrap(샘플) + aggregation(합산)의 합성어 (bagging)

랜덤포레스트가 대표적인 배깅앙상블

주요 hyperparameters

- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!
- n_jobs: CPU 사용 갯수
- max_depth: 깊어질 수 있는 최대 깊이. 과대적합 방지용
- n_estimators: 앙상블하는 트리의 갯수
- max_features: 최대로 사용할 feature의 갯수. 과대적합 방지용
- min_samples_split: 트리가 분할할 때 최소 샘플의 갯수. default=2. 과대적합 방지용

## 부스팅 : 약한학습기를 순서로 학습하여 보완해나가며 학습하는 방식, 오래걸리나 성능이 좋다.

Gradient Boost : 성능이 우수하나 너무너무 느리다.

**주요 Hyperparameter**

- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!
- n_jobs: CPU 사용 갯수
- learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1
- n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100

    러닝레이트와 n_estimators를 맞춰주는 게 좋습니다.

- subsample: 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용
- min_samples_split: 노드 분할시 최소 샘플의 갯수. default=2. 과대적합 방지용

캐글에서 사랑받은 2가지 부스팅(성능도 좋고, 속도도 빨라서)

XGBoost - 빠름

LightGBM - 가장빠름
